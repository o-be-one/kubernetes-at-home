{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Warning</p> <p>WORK IN PROGRESS! At this time, even if most components are ready, project itself and documentation are really experimental.</p> <p>Info</p> <p>This project is made to be a half a playground for learning and experimenting with Kubernetes and half a production to enjoy independancy. If you see any error or enhancement opportunity, please open an issue or a pull request so I can fix it or learn from your experience \u02c6\u02c6.</p>"},{"location":"#why-i-do-that","title":"Why I do that?","text":"<p>I host my own servers since more than 10 years, mostly for non profit organizations and communities. I had to pay monthly fees for non load balanced servers hosted with different providers. I've always been into the topic of self-reliance, and more recently, the topic of self-hosting.</p> <p>Since few years, high speed internet connection came to be more available and affordable. I started to host my own servers at home and wanted to take advantage of a technology stack built to be resilient and scalable at its core. Overall, with UPS batteries, the only SPOF is the Internet connection. I would not say it's cheaper than paying for hosting, but I have more power to host anything and more room to add servers for hard workloads like AI.</p> <p>With my Kubernetes journey, I wanted to be more involved into opensource projects and practice more GitHub. I really hope this little project will help you along your first steps into Kubernetes at Home.</p>"},{"location":"#infrastructure","title":"Infrastructure","text":""},{"location":"#hardware","title":"Hardware","text":"<p>My network is entirely managed using Ubiquiti Unifi products:</p> <ul> <li>1x Unifi Dream Machine</li> <li>2x Unifi Flex Mini</li> </ul> <p>And the cluster hardware is:</p> <ul> <li>3 x Control Plane: Trigkey G4 with Intel N100 (4 cores, 4 threads), 16GB DDR4, and 500GB NVMe</li> <li>2 x Workers: Beelink SER5 with AMD 5560U (6 cores, 12 threads), 16GB DDR4, and 500GB NVMe - I may had a third at some point</li> <li>(future) why not a NAS to keep backups and stateless workload data locally as well</li> </ul>"},{"location":"#network","title":"Network","text":"<p>My Internet connection is 3 Gbps down / 3 Gbps up with a dynamic public IP. So I added a VPN solution made to open HTTP access to public (Tailscale funnel (HTTP(s) only) / Cloudflare Argo Tunnel (HTTP(s) and TCP in beta iirc). We will see later how I deal with that.</p> <p>I've provisionned a dedicated VLAN for the cluster, with DHCP and LAN DNS managed by the Unifi Dream Machine. Folowing are the IPs used:</p> <ul> <li>192.168.200.101 to 192.168.200.103: Control Planes</li> <li>192.168.200.111 to 192.168.200.112: Workers</li> </ul> <p>In cluster network is entirely managed using Cilium.</p>"},{"location":"#software","title":"Software","text":"<p>I've challenged several solutions. My first experience with Kubernetes was with Alpine Linux and k3s, then I moved to a solution available on GitHub to automatically deploy a cluster on Hetzner using Terraform. The solution for Hetzner was using MicroOS from SUSE. When I've looked why they would use MicrOS, I've discovered that some OS are surnamed \"Container OS\" and started to dig into Flatcar and alternatives. Then, a friend told me about another option...</p> <p>And that's how I ended up with Talos OS. Talos is one of the most impressive OS for Kubernetes, described as \"Kubernetes OS\". Talos expose an API that provides similar experience to Kubernetes where you control it using an API. SSH is not installed on it, instead you configure the system using their cli named talosctl. Talos CLI will also helps you to generate config files that can be used in cloud-init or that can be pushed to an idling system waiting for orders.</p>"},{"location":"#security","title":"Security","text":""},{"location":"#mozilla-sops","title":"Mozilla SOPS","text":"<p>Every sensitive information are encrypted using Mozilla SOPS with age plus KSOPS to handle those secrets from the Kubernetes cluster.</p>"},{"location":"#core-components","title":"Core components","text":""},{"location":"#network-cilium","title":"Network (Cilium)","text":"<p>I've often heard about Cilium from a friend who's BPF and XDP passionate, so I wanted to try it. What's interesting with Cilium is that they provide a Kubernetes experience with a strong focus on network, with lot of features and lot of tools. This cluster uses Cilium to manage network and to manage ingress in Gateway API format.</p> <p>Know more about how I setup and maintain Cilium.</p>"},{"location":"#ingress-gateway-api","title":"Ingress (Gateway API)","text":"<p>My first experiences with ingress have been using Ingress Controller from Traefik Labs. I've used Traefik for a while with Docker and really loved it. And then I heard about Gateway API, which is in beta under Cilium. As Kubernetes push to standardize Gateway API, I wanted to try it; and because I'm using Cilium, it was a good fit.</p> <p>Know more about how I setup and maintain Gateway API.</p>"},{"location":"#gitops-argocd","title":"GitOps (ArgoCD)","text":"<p>I've heard about ArgoCD at some point (who into Kubernetes did not?) and I used it in a past professional experience. I really enjoyed the tool so I decided to use it for this cluster. GitOps is interesting as it allows to have a declarative approach to infrastructure and application management from a source of truth.</p> <p>Know more about how I setup and maintain ArgoCD.</p>"},{"location":"#storage-longhorn","title":"Storage (Longhorn)","text":"<p>Longhorn will be our default storage solution. It was challenging to choose one but considering how small is my clsuter and how little experience I have with storage layer, this one looked the most relevant. It's also important to consider that it's still considered in beta and comes with limitation when used with Talos (see here).</p> <p>Know more about how I setup and maintain Longhorn.</p>"},{"location":"#repository-structure","title":"Repository Structure","text":"<p>The project repository is organized as follows:</p> Folder Description <code>apps/</code> Contains the definitions of applications deployed on the cluster. Each subdirectory represents a specific application. <code>core-components/</code> Contains the essential components of the cluster. Includes subdirectories for components such as ArgoCD, Longhorn, etc. <code>infra/</code> Contains infrastructure-related configurations like Cilium-specific configurations. <code>available/</code> Contains apps and core components that are not currently in use but may be of interest to the audience. <code>operators/</code> Contains operators like CloudNativePG for Postgres <code>docs/</code> Contains the project documentation, including this index.md file. <p>Each main folder (<code>apps</code>, <code>core-components</code>, <code>infra</code>) typically contains an <code>applicationset.yaml</code> file. These files are used by ArgoCD to manage the deployment of applications and components in a declarative and automated manner. Sometime you will find a folder named <code>serverside</code>, this one is for ArgoCD Apps that are deployed with serverside strategy.</p> <p>This structure allows for a clear and modular organization of the cluster, facilitating the management and maintenance of various components and applications.</p>"},{"location":"Core%20components/argocd/","title":"ArgoCD","text":""},{"location":"Core%20components/argocd/#access","title":"Access","text":"<p>Reach ArgoCD using kubectl proxy feature: <code>kubectl port-forward svc/argocd-server -n argocd 8080:443</code></p> <p>To push your sops private key, use: <pre><code>cat &lt;your_key_file&gt; | kubectl create secret generic sops-age --namespace=argocd \\\n--from-file=key.txt=/dev/stdin\n</code></pre></p>"},{"location":"Core%20components/argocd/#deleting-argocd-and-all-applications","title":"Deleting ArgoCD and all applications","text":"<p>If ArgoCD is stuck in terminating state, you can try to remove finalizers from all applications:</p> <pre><code>for app in $(kubectl get apps -n argocd -o name); do\n  kubectl patch $app -p '{\"metadata\": {\"finalizers\": null}}' --type merge\ndone\n</code></pre>"},{"location":"Core%20components/cilium/","title":"Cilium","text":""},{"location":"Core%20components/cilium/#prerequisites","title":"Prerequisites","text":"<ul> <li>Cilium CLI: check cilium status</li> </ul>"},{"location":"Core%20components/cilium/#installation","title":"Installation","text":"<p>https://docs.cilium.io/en/stable/installation/k8s-install-helm/</p> <p>Following is related to my first experience with Cilium, but since I've made a Kustomization to handle and deploy Cilium, I'll keep it here for future reference.</p> <pre><code>helm repo add cilium https://helm.cilium.io/\nhelm repo update\n\nhelm install \\\n          cilium \\\n          cilium/cilium \\\n          --version 1.15.5 \\\n          --namespace kube-system \\\n          --set=ipam.mode=kubernetes \\\n          --set=kubeProxyReplacement=true \\\n          --set=securityContext.capabilities.ciliumAgent=\"{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}\" \\\n          --set=securityContext.capabilities.cleanCiliumState=\"{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}\" \\\n          --set=cgroup.autoMount.enabled=false \\\n          --set=ingressController.enabled=true \\\n          --set=ingressController.default=true \\\n          --set=ingressController.loadbalancerMode=shared \\\n          --set=cgroup.hostRoot=/sys/fs/cgroup \\\n          --set=k8sServiceHost=localhost \\\n          --set=k8sServicePort=7445\n</code></pre>"},{"location":"Core%20components/cilium/#upgrade","title":"Upgrade","text":"<p>Reminder: helm can upgrade deployed stack. Example:</p> <pre><code>helm upgrade cilium cilium/cilium --version 1.15.5 \\\n    --namespace kube-system \\\n    --reuse-values \\\n    --set ingressController.loadbalancerMode=shared\n\nkubectl -n kube-system rollout restart deployment/cilium-operator\nkubectl -n kube-system rollout restart ds/cilium\n</code></pre> <p>And to get current values: <code>helm get values cilium -n kube-system -o yaml</code></p>"},{"location":"Core%20components/gateway-api/","title":"Gateway API","text":""},{"location":"Core%20components/gateway-api/#installation","title":"Installation","text":"<p>Warning</p> <p>When upgrading from Gateway API v1.1.0 to v1.2.0, it's crucial to review the changelog. This document outlines significant changes and provides guidance on how to address them during the upgrade process.</p> <p>As I put all of this in the cilium kustomization, you can just run: <code>kubectl kustomize infra/cilium/ --enable-helm | kubectl apply -f -</code></p> <p>To verify if you have all CRDs installed and get their version, run: <code>kubectl get crd -o jsonpath='{range .items[?(@.spec.group==\"gateway.networking.k8s.io\")]}{.metadata.name}: {.metadata.annotations.gateway\\.networking\\.k8s\\.io/bundle-version}{\"\\n\"}{end}'</code></p> <p>Summary of the installation from documentation, can be obsolete:</p> <p>Install Kubernetes Gateway APIs</p> <ul> <li>https://docs.cilium.io/en/latest/network/servicemesh/gateway-api/gateway-api/</li> <li>https://gateway-api.sigs.k8s.io/guides/?h=crds#getting-started-with-gateway-api</li> </ul> <p><pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v1.2.0/config/crd/standard/gateway.networking.k8s.io_gatewayclasses.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v1.2.0/config/crd/standard/gateway.networking.k8s.io_gateways.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v1.2.0/config/crd/standard/gateway.networking.k8s.io_httproutes.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v1.2.0/config/crd/standard/gateway.networking.k8s.io_referencegrants.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v1.2.0/config/crd/standard/gateway.networking.k8s.io_grpcroutes.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v1.2.0/config/crd/experimental/gateway.networking.k8s.io_tlsroutes.yaml\n</code></pre> If your gateway is stuck in \"unknown\" status, it may be because you don't have free IP available in your ip-pool.</p>"},{"location":"Core%20components/gateway-api/#http-to-https-redirect","title":"HTTP to HTTPS redirect","text":"<p>All HTTP routes are automatically redirected to HTTPS using a 302 status code. This redirection is managed by the <code>http-to-https-redirect</code> HTTPRoute, which is located in the same namespace as the shared gateway.</p> <p>If you genuinely need your services to handle HTTP traffic, you have two options:</p> <ol> <li> <p>Remove the redirection route entirely. It's kind of a default behavior that will allow all your apps to be accessible via HTTP.</p> </li> <li> <p>Keep the redirection as the default behavior, but customize specific routes. To do this, simply remove the <code>sectionName: shared-https</code> line from the HTTPRoute specifications of the apps you want to make accessible over HTTP.</p> </li> </ol> <p>This approach provides you with flexibility to manage HTTP and HTTPS traffic according to your specific needs.</p>"},{"location":"Core%20components/longhorn/","title":"Longhorn","text":""},{"location":"Core%20components/longhorn/#prerequisites","title":"Prerequisites","text":"<p>For Longhorn to work, we need to ensure that: - kernel extensions are loaded on Talos nodes (see Image factory (preset)) - we mount /var/lib/longhorn on each node - we use <code>--preserve</code> flag when upgrading Talos nodes to keep data related to persistent volumes</p>"},{"location":"Core%20components/longhorn/#installation","title":"Installation","text":"<p>For all installation information, see Longhorn docs.</p>"},{"location":"Core%20components/tunnel/","title":"Tunnel","text":""},{"location":"Core%20components/tunnel/#public-access","title":"Public access","text":""},{"location":"Core%20components/tunnel/#regular-https-access","title":"Regular HTTPS access","text":"<p>As my network will allow local area network access by default, I've setup a Cloudflare Tunnel to allow public access to selected apps. Cloudflare will hide my IP and it's enabled CDN (proxy) that includes several security features like WAF, DDoS protection, etc.</p>"},{"location":"Core%20components/tunnel/#udp-access","title":"UDP access","text":"<p>I use a Tailscale tunnel to be able to access UDP ports from the outside, using a remote server (VPS for example) as entrypoint and exit node; the exit node part is required for example with games like Core Keeper that only allows to join the game using a \"Game ID\" and it's not possible to join using an IP and port. For games allowing access using IP/dns and port but still in UDP, I use playit.gg. There is examples available in <code>available/apps/core-keeper</code> folder.</p>"},{"location":"Core%20components/tunnel/#tailscale","title":"Tailscale","text":"<p>I had to rent a VPS from OVH / Hetzner / ExtraVM / LowHosting for example so I can use it as entry and exit node. Then simply share IP to my friends, or for Core Keeper the Game ID. Here is a schema of the infrastructure. Thanks to my good friend Lo\u00efc for the drawing.</p> <p></p>"},{"location":"Core%20components/tunnel/#playit","title":"Playit","text":"<p>More straightforward as they handle all the network / VPN part for us including entrypoint.</p>"},{"location":"Infrastructure/installation/","title":"Installation","text":""},{"location":"Infrastructure/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Talos ISO image</li> </ul>"},{"location":"Maintenance/monitoring/","title":"Monitoring","text":""},{"location":"Maintenance/monitoring/#netdata","title":"Netdata","text":"<p>Netdata is a highly performant monitoring and troubleshooting tool for Linux. My choice is motivated by the fact that it's opensource and self-hosted, with deep integration with Kubernetes and deep observability. This one will be used only locally. More information can be found here.</p>"},{"location":"Maintenance/troubleshooting/","title":"Troubleshooting","text":""},{"location":"Maintenance/troubleshooting/#web-returning-404","title":"Web returning 404","text":"<ul> <li>ensure Cilium is running</li> <li>if you have uninstalled Cilium and then reinstalled it, it removed all http routes you had configured</li> </ul>"},{"location":"Maintenance/troubleshooting/#network-issues","title":"Network issues","text":"<ul> <li>check Cilium pods are healthy: <code>kubectl get pods -n kube-system | grep cilium</code></li> <li>use <code>cilium connectivity test</code> to check reachability between nodes</li> <li>this requires privileged namespace, deploy the one in <code>infra/cilium/namespace.yaml</code></li> </ul>"},{"location":"Maintenance/upgrading/","title":"Upgrading","text":""},{"location":"Maintenance/upgrading/#talos","title":"Talos","text":"<p>Because of Longhorn, it's really important to use images from Talos factory that embeds following kernel extensions:</p> <ul> <li>siderolabs/iscsi-tools (required for Longhorn and Synology CSI)</li> <li>siderolabs/util-linux-tools (required for Longhorn)</li> <li>siderolabs/gvisor (security - but not working after Talos 1.8.3 as they use containerd v2)</li> <li>siderolabs/intel-i915 (optional, if Intel iGPU)</li> </ul> <p>Controlplane will need to be upgraded first, then worker nodes. If you have only one controlplane node, don't forget to always use the <code>--preserve</code> flag to keep data related to etcd.</p> <p>Warning</p> <p>Longhorh requires you to use <code>-preserve</code> flag to keep data related to persistent volumes. Don't forget to use it for any nodes involved in Longhorn cluster. Failure to do so will result in data loss.</p> <p>Example of upgrade command: <pre><code>talosctl upgrade -n 192.168.200.112 --image factory.talos.dev/installer/613e1592b2da41ae5e265e8789429f22e121aab91cb4deb6bc3c0b6262961245:v1.7.6 --preserve\n</code></pre></p> <p>References:</p> <ul> <li>Talos upgrade</li> <li>Longhorn docs for Talos</li> <li>Image factory (preset)</li> </ul>"},{"location":"Maintenance/upgrading/#kubernetes","title":"Kubernetes","text":"<p>Upgrading Kubernetes is quite straightforward, just run: <pre><code>talosctl upgrade-k8s --to 1.31.1 -n 192.168.200.101\n</code></pre></p> <p>This should upgrade all nodes in the cluster.</p> <p>References:</p> <ul> <li>Kubernetes upgrade</li> </ul>"},{"location":"Maintenance/upgrading/#cilium","title":"Cilium","text":""},{"location":"Maintenance/upgrading/#longhorn","title":"Longhorn","text":""},{"location":"Security/encryption/","title":"Security","text":""}]}